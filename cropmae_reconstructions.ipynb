{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropped Masked Autoencoders: Reconstruction Demo\n",
    "Inspired by the [MAE Visualization Demo](https://colab.research.google.com/github/facebookresearch/mae/blob/main/demo/mae_visualize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !git clone \"https://github.com/alexandre-eymael/CropMAE\"\n",
    "    !pip3 install \"timm==0.9.16\"\n",
    "    sys.path.append('./CropMAE')\n",
    "else:\n",
    "    !pip3 install -r \"requirements.txt\"\n",
    "\n",
    "!pip3 install \"gdown\" # for downloading files from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import pathlib\n",
    "import gdown\n",
    "\n",
    "from models.SiamMAE import SIAM_MODELS\n",
    "import misc.util as util\n",
    "from data.util import GlobalToLocal, LocalToGlobal, RandomViews, SameViews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_config():\n",
    "    # We load the default configuration for CropMAE\n",
    "    return util.get_args_parser().parse_known_args()[0]\n",
    "\n",
    "def pil_to_tensor(pil_img, img_size=(224, 224)):\n",
    "\n",
    "    img = pil_img.resize(img_size)\n",
    "    img = np.array(img) / 255.\n",
    "\n",
    "    # normalize by ImageNet mean and std\n",
    "    img = img - np.array(IMAGENET_DEFAULT_MEAN)\n",
    "    img = img / np.array(IMAGENET_DEFAULT_STD)\n",
    "\n",
    "    return torch.tensor(img)\n",
    "\n",
    "def show_image(image, title=\"\"):\n",
    "    assert image.shape[2] == 3 # image is [H, W, 3]\n",
    "    plt.imshow(torch.clip((image * np.array(IMAGENET_DEFAULT_STD) + np.array(IMAGENET_DEFAULT_MEAN)) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def download_model_from_google_drive(path=\"https://drive.google.com/file/d/1g-LMXBgtP404LoIwt-F3aA8H5VoFLg75/view?usp=sharing\", name=\"cropmae_vits_16.pt\"):\n",
    "    # The default file corresponds to the vits-16 model used in the paper\n",
    "    return gdown.download(path, name, quiet=False, fuzzy=True)\n",
    "\n",
    "\n",
    "def prepare_model(chkpt_dir, backbone='vits_16'):\n",
    "    model_size, patch_size = backbone.split('_')\n",
    "    patch_size = int(patch_size)\n",
    "    model = SIAM_MODELS[model_size](patch_size=patch_size)\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    status = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(f\"Model Loading Status: {status}\")\n",
    "    return model\n",
    "\n",
    "def run_one_image(img, model, crop_strategy=GlobalToLocal, mask_ratio=0.985):\n",
    "\n",
    "    # Use default CropMAE configuration\n",
    "    config = get_default_config()\n",
    "\n",
    "    # Generate both views `v1`, `v2` from the initial image `img` and make them batch-like\n",
    "    v1, v2 = crop_strategy(config)(img)\n",
    "    v1, v2 = v1.unsqueeze(0), v2.unsqueeze(0)\n",
    "    v1v2 = torch.cat([v1, v2], dim=0).unsqueeze(0)\n",
    "\n",
    "    # Run CropMAE\n",
    "    loss, masked_preds, masked_masks = model(v1v2.float(), mask_ratio=mask_ratio)\n",
    "    \n",
    "    # Reassemble the patches of the reconstruction of v2\n",
    "    rec_v2 = model.unpatchify(masked_preds[0])\n",
    "    rec_v2 = torch.einsum('nchw->nhwc', rec_v2).detach().cpu()\n",
    "\n",
    "    # Create the mask for v2\n",
    "    mask_v2 = masked_masks[0].detach()\n",
    "    mask_v2 = mask_v2.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)\n",
    "    mask_v2 = model.unpatchify(mask_v2)\n",
    "    mask_v2 = torch.einsum('nchw->nhwc', mask_v2).detach().cpu()\n",
    "\n",
    "    # Apply the mask to v2\n",
    "    v2 = torch.einsum('nchw->nhwc', v2)\n",
    "    v2_masked = v2 * (1. - mask_v2)\n",
    "\n",
    "    # Paste the reconstruction of v2 on the original visible patches of v2\n",
    "    im_paste = v2 * (1 - mask_v2) + rec_v2 * mask_v2\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [18, 18]\n",
    "\n",
    "    print(\"Creation of the views:\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    show_image(pil_to_tensor(img), \"Original\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    show_image(v1[0].permute(1, 2, 0), \"V1\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    show_image(v2[0], \"V2\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Reconstruction:\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    show_image(v2_masked[0], f\"V2 Masked at {mask_ratio*100:.2f}%\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    show_image(rec_v2[0], \"V2 Reconstructed from V1\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    show_image(im_paste[0], \"V2 Reconstructed from V1 + Visible\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "img_url = 'https://user-images.githubusercontent.com/11435359/147738734-196fd92f-9260-48d5-ba7e-bf103d29364d.jpg' # fox, from ILSVRC2012_val_00046145\n",
    "# img_url = 'https://user-images.githubusercontent.com/11435359/147743081-0428eecf-89e5-4e07-8da5-a30fd73cc0ba.jpg' # cucumber, from ILSVRC2012_val_00047851\n",
    "pil_img = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "img = pil_to_tensor(pil_img)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained Model\n",
    "\n",
    "**NOTE:** In the paper, the model was trained to predict the normalized pixel values of missing patches. In contrast, here, we train a model to predict the non-normalized pixel values. This approach makes it easier to demonstrate the model's effectiveness in reconstructing the masked view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = download_model_from_google_drive()\n",
    "model = prepare_model(model_local_path, 'vits_16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CropMAE\n",
    "CropMAE works by creating two views $V_1, V_2$ from an input image $I$ by following a particular cropping strategy (refer to the paper).\n",
    "\n",
    "The second view $V_2$ is masked at a very high ratio, typically 98.5%, and reconstructed by the network, which has access to $V_1$ to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make random mask reproducible (comment out to make it change)\n",
    "torch.manual_seed(42)\n",
    "run_one_image(pil_img, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CropMAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
